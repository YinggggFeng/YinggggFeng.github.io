<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yinggggfeng.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yinggggfeng.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-08T04:52:47+00:00</updated><id>https://yinggggfeng.github.io/feed.xml</id><title type="html">blank</title><subtitle>Homepage
</subtitle><entry><title type="html">cmu courses</title><link href="https://yinggggfeng.github.io/courses/" rel="alternate" type="text/html" title="cmu courses" /><published>2023-05-10T00:00:00+00:00</published><updated>2023-05-10T00:00:00+00:00</updated><id>https://yinggggfeng.github.io/courses</id><content type="html" xml:base="https://yinggggfeng.github.io/courses/"><![CDATA[<p>This page contains a list of courses I have taken in cmu and some of my comments to them.</p>

<p>Inspiration for this page came from: <a href="https://wanshenl.me/courses/reviews/">Wan Shen Lim</a>, <a href="https://fanpu.io/courses/">Fan Pu Zeng</a>, etc.</p>

<p>This is not intended to be a very serious review. Many of the opinions are going to be subjective, anecdotal, and highly dependent on my personal preference. Please be very cautious if you are using this page as a source of reference to decide your curriculumn.</p>

<p>Feel free to contact me regarding any questions about these courses.</p>

<ul>
  <li>As a context, I was admitted to <a href="https://www.cmu.edu/dietrich/">Dietrich College</a> in Fall 2020. I had zero knowledge in computer science before coming to cmu. My level of familiarity with math was somewhat below average compared to newly admitted cmu students. I spent most of my high school years and the first year of college focusing on psychology, before I realized my interest in theoretical computer science.</li>
</ul>

<hr />

<h3 id="spring-2023">Spring 2023</h3>
<nobr>
&nbsp; &nbsp; <a href="https://www.andrew.cmu.edu/course/18-330/2023s/">15-330</a> &nbsp; <strong>Introduction to Computer Security</strong>

<details>
  <summary>
  
  <a href="https://15445.courses.cs.cmu.edu/spring2023/">15-445</a> &nbsp; <strong>Database Systems</strong>
  </summary>

  <ul>
    <li>I'm glad that I chose this course to satisfy my system requirement. It covers a lot more interesting real-world database algorithms that I expected. The projects are well-designed, with clear statements of the goals and appropriate level of difficulties. The write-ups include helpful bulletins that guided students to the correct implementations. </li>
  </ul>
</details>

<details>
  <summary>
  
  <a href="https://www.cs.cmu.edu/~15455/index.html">15-455</a> &nbsp; <strong>Undergraduate Complexity Theory</strong>
  </summary>

  <ul>
    <li>Prof. Sutner's teaching has quite a strong personal style. He teaches the algorithms and proofs in a story-telling way, which is fun for students that follow his flow, but at the same time can be a bit confusing for students who don't. The contents are slightly different than a "standard" undergrad complexity theory course. For example, we spent significant time discussing the details of different kinds of automata. I was also surprised (and enjoyed) that we took an entire week learning in-depth about Kolmogorov Complexity. 
    
    The weekly homeworks were relatively long. The total workload of this course is prone to the heavy side. </li>
  </ul>
</details>

<details>
  <summary>
  
  <a href="https://www.cs.cmu.edu/~15850/">15-850</a> &nbsp; <strong>Advanced Algorithms</strong>
  </summary>

  <ul>
    <li> This course felt like an advanced version of 15-451 Algorithm Design and Analysis. We covered a new topic about each week, thus inevitably trading depth for breadth. However, Prof. Gupta managed to convey the importance and attraction for each topic in such limited time. I think this is mostly because of his remarkable enthusiasm -- nearly every algorithms he taught were described as his favorite :)
    
    Also, the class was very interative. Each lecture was filled with students' questions and comments. There were also frequent sharings of relavant notes, papers, and thoughts on the Piazza. In general, I think this course created a really comfortable atmosphere for learning.
    </li>
  </ul>
</details>

<details>
  <summary>
  
  <a href="https://sites.google.com/view/cs15-859winter2023/home?authuser=0">15-859</a> &nbsp; <strong>Topics in Cryptography: Lattice-Based Cryptography</strong>
  </summary>

  <ul>
    <li>It was difficult to get into this course, because Prof. Jain was worried that I might not be prepared enough for the cryptographic topics that he was going to cover. It turned out that he was right -- the contents in this course were very new to me. The lectures felt quite fast-paced and in-depth even after pre-reading every notes from [another similar course](http://people.csail.mit.edu/vinodv/CS294/). But thankfully Prof. Jain used a lot of drawings to visualize the ideas (and was tolerant to my dumb questions.)
    
    The course had two components: the lattice problems and their applications to build cryptogphic tools. I didn't fully relate to the use case of some cryptographic protocols that were discussed towards the end of the semester; however, I really enjoyed the first half of the course, when we covered lattice problems, reductions, and algorithms. </li>
  </ul>
</details>
</nobr>

<hr />

<h3 id="fall-2022">Fall 2022</h3>

<nobr>
<details>
  <summary>
  
  <a href="https://deeplearning.cs.cmu.edu/F22/index.html">11-485</a> &nbsp; <strong>Introduction to Deep Learning</strong>
  </summary>

  <ul>
    <li>I fully agree with [this comment by Max Slater](https://thenumb.at/cmu/). </li>
  </ul>
</details>

<details>
  <summary>
  
  15-312 &nbsp; <strong>Foundations of Programming Languages</strong>
  </summary>

  <ul>
    <li>text</li>
  </ul>
</details>

- [15-451](http://www.cs.cmu.edu/~15451-f22/) &nbsp; **Algorithm Design and Analysis**
- [15-859](http://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/teaching/15859-fall22/index.html) &nbsp; **Algorithms for Big Data**
- 21-325 &nbsp; **Probability**
</nobr>

<hr />

<h3 id="spring-2022">Spring 2022</h3>

<ul>
  <li><a href="https://www.diderot.one/courses/121">15-210</a>   <strong>Parallel and Sequential Data Structures and Algorithms</strong></li>
  <li><a href="https://www.andrew.cmu.edu/user/kpruiksm/15317s22/index.html">15-317</a>   <strong>Constructive Logic</strong></li>
  <li><a href="https://cmu-17-214.github.io/s2022/">17-214</a>   <strong>Principles of Software Construction</strong></li>
  <li>21-259   <strong>Calculus in Three Dimensions</strong></li>
  <li>76-101   <strong>Interpretation and Argument</strong></li>
</ul>

<hr />

<h3 id="fall-2021">Fall 2021</h3>

<ul>
  <li><a href="http://www.cs.cmu.edu/~15150/">15-150</a>   <strong>Functional Programming</strong></li>
  <li>15-251   <strong>Great Ideas in Theoretical Computer Science</strong></li>
  <li>21-122   <strong>Integration and Approximation</strong></li>
  <li>85-213   <strong>Human Information Processing and Artificial Intelligence</strong></li>
  <li>85-219   <strong>Biological Foundations of Behavior</strong></li>
</ul>

<hr />

<h3 id="summer-2021">Summer 2021</h3>

<ul>
  <li><a href="https://www.cs.cmu.edu/~213/">15-213</a>   <strong>Introduction to Computer Systems</strong></li>
  <li>21-241   <strong>Matrices and Linear Transformations</strong></li>
</ul>

<hr />

<h3 id="spring-2021">Spring 2021</h3>

<ul>
  <li><a href="https://www.cs.cmu.edu/~15122/">15-122</a>   <strong>Principles of Imperative Computation</strong></li>
  <li>21-127   <strong>Concepts of Mathematics</strong></li>
  <li>66-118   <strong>Grand Challenge First-Year Seminar: Thinking With Evidence</strong></li>
  <li>76-100   <strong>Reading and Writing in an Academic Context</strong></li>
  <li>85-107   <strong>The Psychology of Video Games</strong></li>
  <li>99-358   <strong>IDeATe: Introduction to the Unity Game Engine</strong></li>
</ul>

<hr />

<h3 id="fall-2020">Fall 2020</h3>

<ul>
  <li><a href="https://www.cs.cmu.edu/~112/">15-112</a>   <strong>Fundamentals of Programming</strong></li>
  <li>36-309   <strong>Experimental Design for Behavioral and Social Sciences</strong></li>
  <li>79-104   <strong>Global Histories: History of Democracy</strong></li>
  <li>80-212   <strong>Arguments and Logical Analysis</strong></li>
  <li>85-102   <strong>Introduction to Psychology</strong></li>
</ul>

<!-- ### Fall 2022
{: .first-course-item #course15859CC } 
- &#11088; 15-859 CC &nbsp; **[Algorithms for Big Data](https://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall22/index.html)**, [David Woodruff](http://www.cs.cmu.edu/~dwoodruf/) 

  Woodruff is one of the giants in sketching and numerical linear algebra, having developed many of its most important algorithms.
  There is even a sklearn function called the [Clarkson-Woodruff transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.clarkson_woodruff_transform.html) that is named after him.

  His teaching is extremely clear as he makes sure to justify and explain every step used in a proof. The analysis for many
  sketching algorithms is highly non-trivial, but Woodruff manages to pull off explaining it in a way that reads like a storybook.
  He cares deeply about the class and the student's learning, and one thing that still amazes me to this day is how he will respond to my
  Piazza questions on a weekend in 2 minutes consistently. I even made a [meme about it](https://t.co/zSg7KwmJkN).

  The homework problems are long but rewarding, and you will become initimately
  familiar with all sorts of linear algebra manipulations and properties. 

  One caveat is that the weekly lectures are 3-hours with a 10-minute break in the middle. Given how dense the lectures are, this can be quite
  taxing, so bring snacks or caffeine if needed.

- &#10084;&#65039; 15-859 OO &nbsp; **[Randomness in Computation](https://www.cs.cmu.edu/~praveshk/randomness.html)**, [Pravesh Kothari](https://www.cs.cmu.edu/~praveshk/)
  {: .course-item #course15859OO } 

  I had limited exposure to most of the topics in this course (mostly from
  Theorist's Toolkit 15-751, Graduate Complexity Theory 15-855, and Graph Theory
  21-484) such as spectral graph theory, expander graphs, derandomization, etc,
  and this course helped to solidify and reinforce my understanding.
  It also proved a lot of things that did not have time to be proved in those earlier classes.
  Overall I felt that Pravesh is a great lecturer, and the topics covered are very interesting
  and applicable. The course was offered for the first time this semester, so there
  were a few rough edges (i.e in the proof of Cheeger's inequality he initially
  did not want to prove it in terms of the Laplacian of the graph to avoid
  introducing new concepts and notations, but doing so ended up being more confusing
  than helpful), but overall it is quite a good class. The homework problems
  are reasonable and the workload is on the lighter side.

- &#11088; 10-708 &nbsp; **[Probablistic Graphical Models](https://andrejristeski.github.io/10708-F22/)**, [Andrej Risteski](https://www.andrew.cmu.edu/user/aristesk/)
  {: .course-item #course10708 } 
  
  This class has a reputation of being one of the hardest ML classes, but I
  think it is actually an excellent class that is very well-taught, so I hope
  that this reputation does not discourage people interested in the content from
  taking it.  The class can be categorized into three module: representation,
  inference, and learning. In the representation module, you will learn about
  how joint distributions of several variables can be represented efficiently by
  various models, taking into factors such as causal relationships.  In the
  inference module, you will learn that sampling from such models is very hard
  in general (assuming $$\P \neq \NP$$), and develop probabilistic ways of 
  sampling from them such as Monte-Carlo Markov-Chain and Variational Inference methods.
  In the final module, you will learn how such models can be fitted to training data.
  Graphical models form the backbone of many modern machine learning techniques like
  generative adversarial networks (GANs) and diffusion models, and the way that
  Andrej teaches all of these topics in a rigorous way to provide a solid
  mathematical understanding of how they work is essential for keeping up
  to date with the state-of-the-art in this field.

- 15-784 &nbsp; **[Foundations of Cooperative AI](https://www.cs.cmu.edu/~15784/)**, [Vincent Conitzer](https://www.cs.cmu.edu/~conitzer/), [Caspar Oesterheld](https://www.andrew.cmu.edu/user/coesterh/), [Tuomas Sandholm](https://www.cs.cmu.edu/~sandholm/)
  {: .course-item #course15784 } 

  This course covered many topics in AI which are typically not covered in a
  machine learning course, such as normal and extensive form games, various
  forms of equilibriums in games, solving for such equilibriums, learning in
  games (regret matching), decision theories, and mechanism design.
  In fact, a lot of it comes from economic theory.
  
  The content was interesting, but I did not enjoy this course as much as I
  would have liked because the way that the content was presented was
  relatively hand-wavey, trading depth for breadth. That said, the course is
  being offered for the first time, so it will probably improve in subsequent
  iterations.

- 10-617 &nbsp; **[Intermediate Deep Learning](https://rsalakhucmu.github.io/10417-22/)**, [Ruslan Salakhutdinov](https://www.cs.cmu.edu/~rsalakhu/)
  {: .course-item #course10617 } 

  Ruslan is one of the household names in the machine learning community (he
  invented the Dropout technique to prevent overfitting which is now standard in
  neural network architectures), and I was very excited to be able to take this
  class with such a legend in the field. I really enjoy his lectures, and he
  made many remarks about what was happening in the field when various
  techniques were being introduced as he was introducing them, which really
  gives you a sense of how the field has evolved over the last few decades from
  a man who has seen and been through it all.

  However, I think the course infrastructure requires more improvement. Some of
  the starter code for the assignments are quite poorly written and contain many
  inconsistencies and wrong/outdated documentation, which leads to a fair amount
  of frustration from students. One particularly annoying inconsistency was how
  the data formats of their starter code were transposed from Homework 1 to
  Homework 2. My guess is that someone tried to update the assignment but
  did not have time to fully go through to fix all the inconsistencies before
  it was released.

  Many people ask about whether they should take 11-485/785 (Introduction to
  Deep Learning) offered by the Language Technologies Institute (LTI), or this
  class offered by the Machine Learning Department (MLD). The main difference is
  that 11-485/785 is more hands-on and practical (most assignments are working
  on Kaggle datasets), whereas 10-417/617 is more
  theoretical.
 
- 10-703 &nbsp; **[Deep Reinforcement Learning and Control](https://cmudeeprl.github.io/703website_f22/)**, [Katerina Fragkiadaki](https://www.cs.cmu.edu/~katef/)
  {: .course-item #course10703 } 
  
  The first half of the course follows the standard Sutton and Barto
  textbook pretty closely, but the second half discusses topics 
  and techniques that are relatively state-of-the-art (think within the last
  3 years). As such, there is not really any reference material other
  than the papers that those techniques were based on. 

  There are usually 2-4 papers that are compulsory readings 
  which are assigned to be read before every lecture. 
  Unfortunately I was a bad student and did not read them beforehand,
  and so after the middle of the semester once the content went
  beyond any standard textbooks, I found it pretty hard to focus and
  understand what is going on in the class, and took very little out
  of lecture. Eventually I had to rewatch them after reading
  through the papers again to be able to properly appreciate it. So if you
  taking this class, please avoid my mistake and do your readings before the
  lecture to save time in the long run!
  
  The homework for this class is really fun as you get to implement
  reinforcement learning algorithms for agents in various OpenAI Gym
  environments. All assignments are done in groups of up to 3, so remember to
  grab a friend or two if you're taking this class.
  
- 21-651 &nbsp; **General Topology**, [Florian Frick](https://www.math.cmu.edu/~ffrick/)
  {: .course-item #course21651 } 

  This class generalizes many concepts that is taught in an undergraduate
  analysis course from metric spaces in $$\mathbb{R}^n$$ to arbitrary
  topological spaces.  It took some time for me to un-learn some of the things
  that I implicitly assumed was just always true, i.e while in a metric space
  you learn that all sequences contains a convergent subsequences in compact
  sets, this is no longer true in arbitrary topological spaces. Much of the content
  have connections and parallels to other deeper areas of mathematics, which
  I found very beautiful.

- 17-603 &nbsp; **Communications for Software Leaders I**, [Dominick (Nick) Frollini](https://www.linkedin.com/in/frollini/)
  {: .course-item #course17603 } 

  This course felt like an MBA class. It is a required class for Masters of Software
  Engineering (MSE) students, and one thing that I did not expect was how much
  the course was geared towards international students (i.e there was quite some
  emphasis on what is appropriate for US customs and norms), which is 
  understandable as most of the MSE students taking the class are international.

  I found the segments about how to give oral presentations useful, especially
  the many tips and things to take note of when presenting.
  However, I don't think I gained as much from other topics, such as those
  concerning written communication.

- 15-604 &nbsp; **Immigration Course**, [Dave Eckhardt](https://www.cs.cmu.edu/~davide/)
  {: .course-item #course15604 } 
  
  Every Monday night, the entire MSCS cohort will gather for this course, where
  Dave will talk about topics ranging from classes to grad school to things to
  do in Pittsburgh. It's usually pretty funny because Dave has a great sense of
  humor, but many of the sessions are also not critically useful so
  attendance does being tapering off in the middle of the semester once
  people start getting busy with school. 
  
Units: 90

This was a really heavy semester for me, mainly because I had to juggle 4 group
projects  (Deep Learning, Probabilistic Graphical Models, Cooperative AI,
Algorithms for Big Data) simultaneously starting from the midpoint of the
semester, and all of them were significant course undertakings which are
anywhere between 30-50% of the final course grade. This is still with
homeworks from all of these classes also being due concurrently,
with the exception of Algorithms for Big Data. 

Fortunately, all the projects turned out relatively well and I was pretty happy
with them, but it definitely took a toll on my physical and mental health. I
ended up only mostly talking to my project groupmates over the last two weeks
of school when all the projects were due. It would probably be wise to learn
from my mistake and make sure that you don't have too many classes with
significant course projects on your schedule to avoid such a situation.

---

### Spring 2022
  {: .first-course-item #course10725 } 
- &#11088; 10-725 &nbsp; **Convex Optimization**, [Yuanzhi
Li](https://www.andrew.cmu.edu/user/yuanzhil/)

  I did not enjoy 10-701 as it covered a lot of content, but did not go into much detail about many topics,
  and I felt like there was no true understanding and everything was very hand-wavey. That
  experience made me hesistant to take any other ML classes.
  
  10-725 changed that for me, as Yuanzhi Li started from first principles and
  rigorously proved how many machine learning algorithms can converge in some
  amount of steps given various assumptions. For instance, just in the 
  second lecture you will learn how gradient descent can converge to the optimum
  up to an epsilon error in a number of states which is proportionate to a
  chosen learning rate, given assumptions on the smoothness of a convex optimization
  landscape. This will then be extended to more complicated settings such as stochastic gradient
  descent, gradient descent with momentum (ADAM), distributed gradient descent, and even
  quantum optimization.

  Yuanzhi Li also understands that the students taking the class have very
  different learning objectives, and therefore the homework contains a mix of
  required and bonus problems, where the bonus problems are usually
  significantly more challenging than the required ones, but are tailored for
  people who really want to get deep into this stuff. The grading policy is
  extremely gentle and you essentially only have to score just half the points
  on the required problems to get an A-. The late day policy is also extremely
  generous (14 days), so it is quite a good class to take if you want some
  flexibility in your schedule.


## Some CMU-Specific Things
This FAQ is mostly geared towards people who are unfamiliar with CMU. -->]]></content><author><name></name></author><summary type="html"><![CDATA[This page contains a list of courses I have taken in cmu and some of my comments to them.]]></summary></entry></feed>